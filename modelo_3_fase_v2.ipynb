{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syatXWlq4njI",
        "outputId": "686a7441-2a0a-46a1-97e6-05f20d18cbcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.2\n",
            "    Uninstalling transformers-4.56.2:\n",
            "      Successfully uninstalled transformers-4.56.2\n",
            "Successfully installed transformers-4.57.0\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from xformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->xformers) (3.0.3)\n",
            "Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers\n",
            "Successfully installed xformers-0.0.32.post2\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.9.11-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.9.13 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.9.14-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.0.32.post2)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.32-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3 (from unsloth)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets!=4.0.*,!=4.1.0,>=3.4.1 (from unsloth)\n",
            "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.10.1)\n",
            "Collecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.23.0,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.19.1)\n",
            "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth)\n",
            "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth) (0.22.1)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.13->unsloth) (0.10.0)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.9.13->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.13->unsloth) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.9.13->unsloth)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.9.11-py3-none-any.whl (317 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.5/317.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.9.14-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.5/256.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.32-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shtab, pyarrow, msgspec, tyro, transformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.48.1 cut_cross_entropy-25.1.1 datasets-4.1.1 msgspec-0.19.0 pyarrow-21.0.0 shtab-1.7.2 transformers-4.56.2 trl-0.23.0 tyro-0.9.32 unsloth-2025.9.11 unsloth_zoo-2025.9.14\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121/torch_stable.html\n",
        "!pip install --upgrade transformers accelerate peft\n",
        "!pip install xformers\n",
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562,
          "referenced_widgets": [
            "67b2c59cdf1a456ba70c698004ce5483",
            "e7621ce32bab47ee95639fb23a53316a",
            "4a4545341cd84e45958e5987b7c3d0ca",
            "ad2ec7d8dbbf41afab444385bde18d4d",
            "a7c27e8e1b7b44e79878b53e19ffa201",
            "9aec82072e2d48d8b345595a00604c41",
            "783a74bbd0a74379a97e5b44aee4220c",
            "0018df3ed2e249ac8626cbf7d4700b0e",
            "16a247a960f6452ea67e55f49516794e",
            "ccd67c7417fc411cb45862fe97202a16",
            "5108ee3669e54212ba138ed1e1fb47e6",
            "407472a28c424216a0903b6cbc80b78f",
            "90a7c06214784a01a03d769cfa5ebb3d",
            "e2fc80ea984b49ce841e284820bb3e22",
            "c234c0b048554ce28df092266ef86c03",
            "f3524827cecb476eb195ba04a43186a6",
            "8ea483391145492484d010ca3fe91a72",
            "d07a535d5f464ba1a54df3ddff049f79",
            "a113e528daf9457483765ec3690cf609",
            "4e5dd6ed658e43eab8a4a220fdfd70f1",
            "6727674b7d3f4b40bfa9b073bdf5d9a0",
            "6990313dd6c94d6a8d8568d099888141",
            "cda19b619f864dc3b9d7f0e4cc30e9c7",
            "d35ecb7a49874196a134ea42c9749344",
            "4fe2bb33c5b8477c846d1223b6e5e4fa",
            "dfe6de4e4cd6480f9696496d3ea502b9",
            "2ca45a4aeafe414e97be31abc0bb75d5",
            "6f9a7c4cc0b74a399e900806fcf3757f",
            "57a9103ac2904389a8268d8c524ca0ff",
            "3dd4436b319442699c2c24ebec1543f5",
            "7f6547f7a3284eca926f3ab961373213",
            "170e892c7f6e4ecb8df12f1b69aeba79",
            "c4457468a00241be96e30a89acb99f2d",
            "21d3fed8a8dd40eca566e73e9b5d5c47",
            "ddb3da5231984175aa039b3b1d16f4ed",
            "bebe85688e7247ae92f2cb160de6ceaf",
            "667bbeabfc214b31a6cc33dbd80d729e",
            "38c00318e76d40bcb05e26fc739ac8b2",
            "38733db12b414cc5b3b0c65df404b24f",
            "db9e2e6356414cabae250d35c15759b2",
            "405c7c9e888949c78018a0750f3d6bfb",
            "1c0da59a679e4ef2a1fa6d108845d972",
            "f14f3a3977b7423bb1b468559fc9ceb5",
            "290a27826ad2418da4a16714aa61c8c2",
            "56a8b9e1e3734908beb131ed02e3d7e1",
            "42f580e2a2b8453dbca1250ca94f04fd",
            "cd9f638fafd2486fa301b70ec5e3fc03",
            "3cf4c259ae9d49b6a934646095cc7e78",
            "19f94a57744b46e798116eda9cc6472b",
            "de1a6a4a5e004641a8da701030ef3816",
            "95f8d63aba904e238484626c3b796f06",
            "60cbcbc50374455a96a996037aa9acbe",
            "9e8e35bca0794afe8a5a1573d82e83f6",
            "d866481f0d1349179b2e319c0f8ddc1d",
            "6b1bb48ce9704ff5a9ae4b37601985f1",
            "c9a51c19abef47ea8863ec2dd6a0bff2",
            "92175674c0274a3ea5a6969d177ae06c",
            "f04ec878ade64e29bbdd744e7697cca3",
            "9557a7ff4d4f44619fe4377f74d5aa85",
            "7d8105d1364644c79bc04ecebb94e21a",
            "43d0d580904441f9b1f43472aaf8d3f4",
            "4cc602d9f1ca49cdb76ee31c2b8c467c",
            "0d46943026274f4e8abc7a450fd9c876",
            "56810808b86a4b929a11d3c58e4d2533",
            "a7cac92423514bd79d2326a00c17bcde",
            "247ef3d57b7d4d1e844bbce0fc82e293",
            "0d23cbfaef23495d8e85bc94e428392b",
            "a057c115b3a94e9eb7e9dbb9798f46d5",
            "6bc7f82484444b4e947aa636b618787c",
            "e5a5b7d82241493f902d8997de8ad52a",
            "a6201c167c704768a53a434254f46c90",
            "b09c07752a7d42cea57855a081453c69",
            "555f4da134654d5bb3b35aacda2a5176",
            "ae5d2016252940c28f82915a9a39e189",
            "1159644e07c64dde912cf1d1e039a647",
            "3cbcdb602c81494d94a5b34bc1524014",
            "6c865cd1548e44128af7b34c83f6a516",
            "e60c520dda1c4659b3d0168ef787622b",
            "9b764aee5029453697f0ca65db25c9e4",
            "2ec8da8c756d4391b69b6f97f8a0042d",
            "b96684d46827433d9c1c6036431834b1",
            "80d18a40d7c640bfb993dd66b949529c",
            "efeacd3658d4402f837dd4052641412a",
            "851ec34d0dda4d9b9237c813f6c24eef",
            "ff23658647b046d38fea8048e0b2b596",
            "f033d7378cd04eef8e7443dc755d2068",
            "edc9fcdcbaef4f758cdb3dd1347c34ea",
            "4121db4583e244b584d1e34c2885ec99",
            "f1e3486cf2394cbfa97781a024587ceb",
            "77ca42d67aa84072866424ac7e58df01",
            "91a049960b904f34bb12de46ebfa2e6d",
            "006a16ffc5774f68822597de6e180916",
            "d16433d3256d4e6fa2e8d05283a16ddf",
            "5053435a58e94fad8a9c954011af2d26",
            "a5296d235cac4aac9848a2ab96274d60",
            "3d7c4aec249b417e9190ee8a4e062d85",
            "6ddcfe880a8f4e00be6a3fb9f96b4d76",
            "6c22902b3d9e405b96ebb6ec98931610",
            "17e4b27b853d4aad83249e0ffc6808d4",
            "e8e2f15515914abbbaef1026ba76b03b",
            "52c5817fbd7c449ab6864abfcb97fb48",
            "0d25d5742f124e33beb5e91330b5baa6",
            "0eddc8cac36c4672a3a5b7fb0db0bd7c",
            "3742b372ae4e40e3b009e6a78c8ba7ab",
            "765ad7e2129b4daa837ec1ded5313639",
            "202ab485e51f475795968030ca44ae9f",
            "5de560d650744c6284c9e674c6c2e22d",
            "52b8d70dfa314b4bbd56a7f26e8cf125",
            "8fa7a2395026405aaf37b9a8600a5cde",
            "7a63239fb0184e7fab7d3a885ebf6778",
            "2b84319c59964a419eded136818906e5",
            "61323d7fa2ca43d6a828218f623a3347",
            "26a0cc8966594d9f97bbace35bf029d5",
            "edf548415b9745e8a81aecb5c42beae0",
            "5dbc7c97948f4c308087549d2e08c282",
            "6fc4ddd13f3a42be9bc7f08ff4f8923e",
            "319d4fe1122145e18e4b16fd1cda0e58",
            "256982940fb74bfe96c6e1827ff13cb2",
            "18f5148da34d450fb3ed05c6831aa107",
            "1d9469e38d7246f7b4543ca454ddb9e3",
            "b2cdba56319c4539b7ce5f81bff48d74",
            "d19456e6a6b148eea5f5a8a3b64a2421",
            "36299f38a3594fdca0cc42a54bbdeb71",
            "603251e26d41451baeb39598fd75b84d",
            "1061a07267e044bf8a7b2f19a4afa101",
            "0318edc275114e68a6cfd8e42279e32e",
            "3d1a44e3cac04262afb8da6fac317146",
            "f2a9173dadae406e8150e029124fb053",
            "551ce940e32c4f259967ac52ee8b9232",
            "12a08b913d584d6e8bbcb3ba337a24da",
            "7f2b819265c34402b16a76930db3bbc3",
            "922eaedeee2d4a5199302d36f86f160a"
          ]
        },
        "id": "Z0-lGZxL4M5P",
        "outputId": "07325eef-50a3-439a-db66-045d8fd4d321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.11: Fast Mistral patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67b2c59cdf1a456ba70c698004ce5483"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "407472a28c424216a0903b6cbc80b78f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cda19b619f864dc3b9d7f0e4cc30e9c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.71G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21d3fed8a8dd40eca566e73e9b5d5c47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56a8b9e1e3734908beb131ed02e3d7e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/194 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9a51c19abef47ea8863ec2dd6a0bff2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d23cbfaef23495d8e85bc94e428392b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e60c520dda1c4659b3d0168ef787622b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1e3486cf2394cbfa97781a024587ceb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/572 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8e2f15515914abbbaef1026ba76b03b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b84319c59964a419eded136818906e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/318 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d19456e6a6b148eea5f5a8a3b64a2421"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.9.11 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "import html\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "selectded_model = \"unsloth/Phi-3-medium-4k-instruct\"\n",
        "\n",
        "fourbit_models = [selectded_model]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = selectded_model,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "57ixl4Ps8-bn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def solve_relative_path(file_path: str) -> str:\n",
        "    project_root: str = os.path.dirname(os.getcwd())\n",
        "    data_path: str = os.path.abspath(os.path.join(project_root, file_path))\n",
        "    return data_path\n",
        "\n",
        "def read_json_file(file_path: str) -> list[tuple[str, str]]:\n",
        "    training_data = []\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        buffer: str = \"\"\n",
        "        for line in file:\n",
        "            buffer += line.strip()\n",
        "            try:\n",
        "                item: dict = json.loads(buffer)\n",
        "                buffer = \"\"\n",
        "                title: str = clean_text(item.get(\"title\", \"\"))\n",
        "                content: str = clean_text(item.get(\"content\", \"\"))\n",
        "                if title and content:\n",
        "                    training_data.append((title, content))\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    return training_data\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = html.unescape(text)\n",
        "    text = re.sub(r\"--.*\", \"\", text)\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def verify_gpu_availability() -> None:\n",
        "    is_cuda_available = torch.cuda.is_available()\n",
        "    print(f\"CUDA disponÃ­vel? {is_cuda_available}\")\n",
        "\n",
        "    if is_cuda_available:\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        print(f\"NÃºmero de GPUs disponÃ­veis: {gpu_count}\")\n",
        "        for i in range(gpu_count):\n",
        "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "\n",
        "def format_data_for_training_with_phi35(data: list[tuple[str,str]]) -> list[str]:\n",
        "    formatted_data = []\n",
        "    for title, content in data:\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"what is the minimum recommended age for reading {title} and why\"},\n",
        "            {\"role\": \"assistant\", \"content\": content},\n",
        "        ]\n",
        "        formatted_data.append({\"text\": tokenizer.apply_chat_template(messages, tokenize=False)})\n",
        "    return formatted_data\n",
        "\n",
        "def format_data_generator(file_path: str):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        buffer = \"\"\n",
        "        for line in f:\n",
        "            buffer += line.strip()\n",
        "            try:\n",
        "                item = json.loads(buffer)\n",
        "                buffer = \"\"\n",
        "                title = clean_text(item.get(\"title\", \"\"))\n",
        "                content = clean_text(item.get(\"content\", \"\"))\n",
        "                if title and content:\n",
        "                    messages = [\n",
        "                        {\"role\": \"user\", \"content\": f\"what is the minimum recommended age for reading {title} and why\"},\n",
        "                        {\"role\": \"assistant\", \"content\": content},\n",
        "                    ]\n",
        "                    yield {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
        "            except json.JSONDecodeError:\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b779aaf8ebd249aa8869a9c2665508a7",
            "e4c5e7020a5349a9bcd8b8de3d4efd52",
            "3d8e22bb00ff413094d8440ef06e6452",
            "bba94eb381bd43fa8fd43d2aba86beac",
            "52ba824028c04073bea59018cd266bf0",
            "c77f933fc847461cbf1e1ae54ed4df15",
            "b46ea7d385984ba4bfaaa72a7cd6cfec",
            "870dcc31f47d4135ac099344748f6aae",
            "b023057acefb49979c95d8f4ccca5f70",
            "fd8a68204e3a402584b5527a09491ef5",
            "6f23cde6408f471c83f1a7098dc07558",
            "f2942bc75c394c2b94875167f0c6ab59",
            "f1aa86cf619140cfb521a9fd6ab1e055",
            "0097962490cb46d3be318abf88631009",
            "ac34242fafa645d892109d0864955d32",
            "380e13793b97486da8565eef191fd6fb",
            "32e13b129d4e455bb89dc466f5c70ee2",
            "9219400a6bb849e2802dfbc9cf37e3ea",
            "bbebf0990f1e4dba840d4fe4f7cd0d2a",
            "12dd0d6739e644ee9e968bc06acd30ce",
            "17e543ce53d84f51a61e9de2c935081a",
            "f7262d3c09754081950dd50046a3aaf5",
            "2641bcb1f38c46daa9f09d1be75857c5",
            "9ec9ca61753146aaad7348743e4f2e02",
            "b547b8a858024adf9b4c1d5ac271de93",
            "6c7272d9aca44da5b35add0d6ba2b2e6",
            "2854eacaf25345c1b4d1227fef89558e",
            "49ede8677c2e45c59b31e0b88dee751e",
            "d8398239a700485f97f80dafeb5d5e96",
            "0f8e2d73068548089becf52b4da14012",
            "12db5389394f4c6092f9bcf91bc14208",
            "a7496a48ed7d4734bcdd1ea714c8c13f",
            "a049840d85484207aadf9b1f127632c2"
          ]
        },
        "id": "-usiKnrj9F8Q",
        "outputId": "dc1926d7-1c35-4541-d39f-aed5b6a87efa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA disponÃ­vel? True\n",
            "NÃºmero de GPUs disponÃ­veis: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b779aaf8ebd249aa8869a9c2665508a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2942bc75c394c2b94875167f0c6ab59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/1389915 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2641bcb1f38c46daa9f09d1be75857c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando o fine-tuning...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,389,915 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 65,536,000 of 14,025,774,080 (0.47% trained)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  5/100 01:24 < 44:50, 0.04 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.892000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.192500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.645800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 38:57, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.892000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.192500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.645800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.786100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.553600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.717100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.404400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.229400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.109400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.393100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.156000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.206700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.146400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.104900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.022700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.197600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.124100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.192900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.850200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.958100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.151400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.120800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.119900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.140300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.214500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.970600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.038000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.886100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.126600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.927700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.183100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.255700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.764300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.950300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.102700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.104400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.051600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.156900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>2.187900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.003100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.990300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.965700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.012000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.199000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.950300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>2.085300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.545300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.200700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.010200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.029700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.882200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.977800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.073600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.761000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.165200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.974600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.129600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.170200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.832600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>2.026500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.063200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.127800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.024400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.906000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.937900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>2.126000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.092700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>2.176900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.131800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.049700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.994500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.194300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>2.093800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>2.020200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>2.030200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.903500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.969300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>2.070600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>2.128600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.851700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.956100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.138100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.851700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.755600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>2.193000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>2.055100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.103200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.752900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.692100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>2.260800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.081100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning concluÃ­do!\n"
          ]
        }
      ],
      "source": [
        "verify_gpu_availability()\n",
        "\n",
        "dataset_path = solve_relative_path(\"/content/drive/MyDrive/Colab Notebooks/trn.json\")\n",
        "dataset = Dataset.from_generator(format_data_generator, gen_kwargs={\"file_path\": dataset_path})\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    max_steps=100,\n",
        "    warmup_steps=5,\n",
        "    logging_steps=1,\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset=dataset.select(range(200)),\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = training_args,\n",
        "    packing = True,\n",
        ")\n",
        "\n",
        "print(\"Iniciando o fine-tuning...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Fine-tuning concluÃ­do!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW8DrqpOVhi4",
        "outputId": "5a02cc27-b706-4251-b662-968b553fc022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TESTE DE INFERÃŠNCIA ---\n",
            "what is the minimum recommended age for reading Jane's Battleships of the 20th Century and why? \"Jane's Battleships of the 20th Century is a must for anyone interested in the history of the battleship. The book is well-illustrated and well-written, and the author's knowledge of the subject is evident. The book is a good reference for the history of the battleship, and it is a good read for anyone interested in the subject.\"\n"
          ]
        }
      ],
      "source": [
        "title_for_testing = \"Jane's Battleships of the 20th Century\"\n",
        "message_for_testing = [\n",
        "    {\"role\": \"user\", \"content\": f\"what is the minimum recommended age for reading {title_for_testing} and why?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(message_for_testing, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=256, use_cache=True)\n",
        "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"\\n--- TESTE DE INFERÃŠNCIA ---\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Overall Training Stats ---\")\n",
        "print(f\"Total Steps Completed: {trainer_stats.global_step}\")\n",
        "print(f\"Average Training Loss: {trainer_stats.training_loss:.4f}\") # Format for readability\n",
        "\n",
        "print(\"\\n--- Detailed Log History ---\")\n",
        "# Access the detailed log from the trainer's state\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "for log in log_history:\n",
        "    # We check if 'loss' is in the log, as the last entry might be the final training metrics\n",
        "    if 'loss' in log:\n",
        "        step = log['step']\n",
        "        loss = log['loss']\n",
        "        lr = log['learning_rate']\n",
        "        print(f\"Step: {step:<5} | Loss: {loss:.4f} | Learning Rate: {lr:.6f}\")"
      ],
      "metadata": {
        "id": "ooDJ1pjuaLu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e7fea6-5df1-4240-e834-04f6cff50ace"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Overall Training Stats ---\n",
            "Total Steps Completed: 100\n",
            "Average Training Loss: 2.1246\n",
            "\n",
            "--- Detailed Log History ---\n",
            "Step: 1     | Loss: 3.8920 | Learning Rate: 0.000000\n",
            "Step: 2     | Loss: 3.1925 | Learning Rate: 0.000040\n",
            "Step: 3     | Loss: 3.6458 | Learning Rate: 0.000080\n",
            "Step: 4     | Loss: 3.0130 | Learning Rate: 0.000120\n",
            "Step: 5     | Loss: 2.7861 | Learning Rate: 0.000160\n",
            "Step: 6     | Loss: 2.5536 | Learning Rate: 0.000200\n",
            "Step: 7     | Loss: 2.7171 | Learning Rate: 0.000200\n",
            "Step: 8     | Loss: 2.4044 | Learning Rate: 0.000200\n",
            "Step: 9     | Loss: 2.2294 | Learning Rate: 0.000200\n",
            "Step: 10    | Loss: 2.1094 | Learning Rate: 0.000199\n",
            "Step: 11    | Loss: 2.3931 | Learning Rate: 0.000199\n",
            "Step: 12    | Loss: 2.1560 | Learning Rate: 0.000198\n",
            "Step: 13    | Loss: 2.2067 | Learning Rate: 0.000197\n",
            "Step: 14    | Loss: 2.0169 | Learning Rate: 0.000197\n",
            "Step: 15    | Loss: 2.1464 | Learning Rate: 0.000196\n",
            "Step: 16    | Loss: 2.1049 | Learning Rate: 0.000195\n",
            "Step: 17    | Loss: 2.0227 | Learning Rate: 0.000193\n",
            "Step: 18    | Loss: 2.1976 | Learning Rate: 0.000192\n",
            "Step: 19    | Loss: 2.1241 | Learning Rate: 0.000191\n",
            "Step: 20    | Loss: 2.1929 | Learning Rate: 0.000189\n",
            "Step: 21    | Loss: 1.8502 | Learning Rate: 0.000188\n",
            "Step: 22    | Loss: 1.9581 | Learning Rate: 0.000186\n",
            "Step: 23    | Loss: 2.1514 | Learning Rate: 0.000185\n",
            "Step: 24    | Loss: 2.1208 | Learning Rate: 0.000183\n",
            "Step: 25    | Loss: 2.1199 | Learning Rate: 0.000181\n",
            "Step: 26    | Loss: 2.1403 | Learning Rate: 0.000179\n",
            "Step: 27    | Loss: 2.2145 | Learning Rate: 0.000177\n",
            "Step: 28    | Loss: 1.9706 | Learning Rate: 0.000175\n",
            "Step: 29    | Loss: 2.1125 | Learning Rate: 0.000172\n",
            "Step: 30    | Loss: 2.0870 | Learning Rate: 0.000170\n",
            "Step: 31    | Loss: 2.0380 | Learning Rate: 0.000168\n",
            "Step: 32    | Loss: 1.8861 | Learning Rate: 0.000165\n",
            "Step: 33    | Loss: 2.1266 | Learning Rate: 0.000163\n",
            "Step: 34    | Loss: 1.9277 | Learning Rate: 0.000160\n",
            "Step: 35    | Loss: 2.1500 | Learning Rate: 0.000157\n",
            "Step: 36    | Loss: 2.1831 | Learning Rate: 0.000155\n",
            "Step: 37    | Loss: 2.2557 | Learning Rate: 0.000152\n",
            "Step: 38    | Loss: 1.7643 | Learning Rate: 0.000149\n",
            "Step: 39    | Loss: 1.9503 | Learning Rate: 0.000146\n",
            "Step: 40    | Loss: 2.1027 | Learning Rate: 0.000143\n",
            "Step: 41    | Loss: 2.1044 | Learning Rate: 0.000140\n",
            "Step: 42    | Loss: 2.0516 | Learning Rate: 0.000137\n",
            "Step: 43    | Loss: 2.1569 | Learning Rate: 0.000134\n",
            "Step: 44    | Loss: 2.1879 | Learning Rate: 0.000131\n",
            "Step: 45    | Loss: 2.0031 | Learning Rate: 0.000128\n",
            "Step: 46    | Loss: 1.9903 | Learning Rate: 0.000125\n",
            "Step: 47    | Loss: 1.9657 | Learning Rate: 0.000121\n",
            "Step: 48    | Loss: 2.0120 | Learning Rate: 0.000118\n",
            "Step: 49    | Loss: 2.1990 | Learning Rate: 0.000115\n",
            "Step: 50    | Loss: 1.9503 | Learning Rate: 0.000112\n",
            "Step: 51    | Loss: 2.0853 | Learning Rate: 0.000108\n",
            "Step: 52    | Loss: 1.5453 | Learning Rate: 0.000105\n",
            "Step: 53    | Loss: 2.2007 | Learning Rate: 0.000102\n",
            "Step: 54    | Loss: 2.0102 | Learning Rate: 0.000098\n",
            "Step: 55    | Loss: 2.0218 | Learning Rate: 0.000095\n",
            "Step: 56    | Loss: 2.0297 | Learning Rate: 0.000092\n",
            "Step: 57    | Loss: 1.8822 | Learning Rate: 0.000088\n",
            "Step: 58    | Loss: 1.9778 | Learning Rate: 0.000085\n",
            "Step: 59    | Loss: 2.0736 | Learning Rate: 0.000082\n",
            "Step: 60    | Loss: 1.7610 | Learning Rate: 0.000079\n",
            "Step: 61    | Loss: 2.1652 | Learning Rate: 0.000075\n",
            "Step: 62    | Loss: 1.9746 | Learning Rate: 0.000072\n",
            "Step: 63    | Loss: 2.2128 | Learning Rate: 0.000069\n",
            "Step: 64    | Loss: 2.1296 | Learning Rate: 0.000066\n",
            "Step: 65    | Loss: 2.1702 | Learning Rate: 0.000063\n",
            "Step: 66    | Loss: 2.1350 | Learning Rate: 0.000060\n",
            "Step: 67    | Loss: 1.8326 | Learning Rate: 0.000057\n",
            "Step: 68    | Loss: 2.0265 | Learning Rate: 0.000054\n",
            "Step: 69    | Loss: 2.0632 | Learning Rate: 0.000051\n",
            "Step: 70    | Loss: 2.1278 | Learning Rate: 0.000048\n",
            "Step: 71    | Loss: 2.0244 | Learning Rate: 0.000045\n",
            "Step: 72    | Loss: 1.9060 | Learning Rate: 0.000043\n",
            "Step: 73    | Loss: 1.9379 | Learning Rate: 0.000040\n",
            "Step: 74    | Loss: 2.1260 | Learning Rate: 0.000037\n",
            "Step: 75    | Loss: 2.0927 | Learning Rate: 0.000035\n",
            "Step: 76    | Loss: 2.1769 | Learning Rate: 0.000032\n",
            "Step: 77    | Loss: 2.1318 | Learning Rate: 0.000030\n",
            "Step: 78    | Loss: 2.0497 | Learning Rate: 0.000028\n",
            "Step: 79    | Loss: 1.9945 | Learning Rate: 0.000025\n",
            "Step: 80    | Loss: 2.1943 | Learning Rate: 0.000023\n",
            "Step: 81    | Loss: 2.0938 | Learning Rate: 0.000021\n",
            "Step: 82    | Loss: 2.0202 | Learning Rate: 0.000019\n",
            "Step: 83    | Loss: 2.0302 | Learning Rate: 0.000017\n",
            "Step: 84    | Loss: 1.9035 | Learning Rate: 0.000015\n",
            "Step: 85    | Loss: 1.9693 | Learning Rate: 0.000014\n",
            "Step: 86    | Loss: 2.0706 | Learning Rate: 0.000012\n",
            "Step: 87    | Loss: 2.1286 | Learning Rate: 0.000011\n",
            "Step: 88    | Loss: 1.8517 | Learning Rate: 0.000009\n",
            "Step: 89    | Loss: 1.9561 | Learning Rate: 0.000008\n",
            "Step: 90    | Loss: 2.1381 | Learning Rate: 0.000007\n",
            "Step: 91    | Loss: 1.8517 | Learning Rate: 0.000005\n",
            "Step: 92    | Loss: 1.7556 | Learning Rate: 0.000004\n",
            "Step: 93    | Loss: 2.1930 | Learning Rate: 0.000003\n",
            "Step: 94    | Loss: 2.0551 | Learning Rate: 0.000003\n",
            "Step: 95    | Loss: 2.1032 | Learning Rate: 0.000002\n",
            "Step: 96    | Loss: 1.7529 | Learning Rate: 0.000001\n",
            "Step: 97    | Loss: 1.6921 | Learning Rate: 0.000001\n",
            "Step: 98    | Loss: 1.6616 | Learning Rate: 0.000000\n",
            "Step: 99    | Loss: 2.2608 | Learning Rate: 0.000000\n",
            "Step: 100   | Loss: 2.0811 | Learning Rate: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.DataFrame(log_history)\n",
        "df.dropna(subset=['loss'], inplace=True)\n",
        "plt.plot(df['step'], df['loss'])\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gQ3AEc1Qtgkj",
        "outputId": "29d33d59-49eb-44cd-d6b5-1c169bc34431"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}